#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæƒ…ç»ªè¯†åˆ«API
æä¾›æœ€ç®€å•æ˜“ç”¨çš„æƒ…ç»ªè¯†åˆ«æ¥å£
"""

import time
import logging
from typing import Dict, Optional, Callable
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QuickEmotionAPI:
    """å¿«é€Ÿæƒ…ç»ªè¯†åˆ«API"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¿«é€ŸAPI"""
        self.detector = None
        self._init_detector()
    
    def _init_detector(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        try:
            from unified_emotion_detector import UnifiedEmotionDetector
            self.detector = UnifiedEmotionDetector()
            logger.info("âœ… æƒ…ç»ªè¯†åˆ«APIåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ APIåˆå§‹åŒ–å¤±è´¥: {e}")
            self.detector = None
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥APIæ˜¯å¦å‡†å¤‡å°±ç»ª"""
        if not self.detector:
            return False
        
        status = self.detector.get_status()
        return any(status['modules_ready'].values())
    
    def quick_record_and_analyze(self, duration: int = 5) -> Dict:
        """ä¸€é”®å½•åˆ¶å¹¶åˆ†ææƒ…ç»ª"""
        if not self.is_ready():
            return {'success': False, 'error': 'APIæœªå‡†å¤‡å°±ç»ª'}
        
        print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ï¼Œæ—¶é•¿: {duration}ç§’")
        print("ğŸ’¡ è¯·å¯¹ç€æ‘„åƒå¤´è¯´è¯ï¼Œç³»ç»Ÿå°†åˆ†ææ‚¨çš„è¯­éŸ³å’Œè¡¨æƒ…")
        
        results = []
        
        def result_handler(result):
            results.append(result)
            print(f"ğŸ¯ æ£€æµ‹åˆ°æƒ…ç»ª: {result['final_emotion']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
        
        self.detector.set_result_callback(result_handler)
        
        if not self.detector.start_recording(duration):
            return {'success': False, 'error': 'å½•åˆ¶å¯åŠ¨å¤±è´¥'}
        
        time.sleep(duration + 1)
        self.detector.stop_recording()
        
        if results:
            return self._generate_summary(results)
        else:
            return {'success': False, 'error': 'æœªæ£€æµ‹åˆ°ä»»ä½•æƒ…ç»ªç»“æœ'}
    
    def analyze_text(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬æƒ…ç»ª"""
        if not self.is_ready():
            return {'success': False, 'error': 'APIæœªå‡†å¤‡å°±ç»ª'}
        
        result = self.detector.analyze_text(text)
        
        if 'error' in result:
            return {'success': False, 'error': result['error']}
        
        return {
            'success': True,
            'emotion': result['emotion'],
            'confidence': result['confidence'],
            'text': text,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def analyze_audio_file(self, audio_file: str) -> Dict:
        """
        åˆ†æéŸ³é¢‘æ–‡ä»¶æƒ…ç»ª
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.is_ready():
            return {
                'success': False,
                'error': 'APIæœªå‡†å¤‡å°±ç»ª'
            }
        
        result = self.detector.analyze_audio_file(audio_file)
        
        if 'error' in result:
            return {
                'success': False,
                'error': result['error']
            }
        
        return {
            'success': True,
            'emotion': result['emotion'],
            'confidence': result['confidence'],
            'file': audio_file,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def analyze_image_file(self, image_file: str) -> Dict:
        """
        åˆ†æå›¾åƒæ–‡ä»¶æƒ…ç»ª
        
        Args:
            image_file: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.is_ready():
            return {
                'success': False,
                'error': 'APIæœªå‡†å¤‡å°±ç»ª'
            }
        
        result = self.detector.analyze_image_file(image_file)
        
        if 'error' in result:
            return {
                'success': False,
                'error': result['error']
            }
        
        return {
            'success': True,
            'emotion': result['emotion'],
            'confidence': result['confidence'],
            'file': image_file,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def _generate_summary(self, results: list) -> Dict:
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        if not results:
            return {'success': False, 'error': 'æ— ç»“æœæ•°æ®'}
        
        emotion_counts = {}
        confidence_scores = []
        
        for result in results:
            emotion = result['final_emotion']
            confidence = result['confidence']
            
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            confidence_scores.append(confidence)
        
        dominant_emotion = max(emotion_counts, key=emotion_counts.get)
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        
        return {
            'success': True,
            'dominant_emotion': dominant_emotion,
            'average_confidence': avg_confidence,
            'emotion_distribution': emotion_counts,
            'total_detections': len(results),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'summary': f"ä¸»è¦æƒ…ç»ª: {dominant_emotion} (å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f})"
        }
    
    def get_status(self) -> Dict:
        """è·å–APIçŠ¶æ€"""
        if not self.detector:
            return {
                'ready': False,
                'error': 'æ£€æµ‹å™¨æœªåˆå§‹åŒ–'
            }
        
        status = self.detector.get_status()
        
        return {
            'ready': self.is_ready(),
            'modules': status['modules_enabled'],
            'modules_ready': status['modules_ready']
        }


# å…¨å±€APIå®ä¾‹
_api_instance = None

def get_emotion_api() -> QuickEmotionAPI:
    """è·å–å…¨å±€APIå®ä¾‹"""
    global _api_instance
    if _api_instance is None:
        _api_instance = QuickEmotionAPI()
    return _api_instance


def quick_emotion_check(duration: int = 5) -> Dict:
    """å¿«é€Ÿæƒ…ç»ªæ£€æµ‹ï¼ˆæœ€ç®€å•çš„æ¥å£ï¼‰"""
    api = get_emotion_api()
    return api.quick_record_and_analyze(duration)


def analyze_text_emotion(text: str) -> Dict:
    """å¿«é€Ÿæ–‡æœ¬æƒ…ç»ªåˆ†æ"""
    api = get_emotion_api()
    return api.analyze_text(text)


def analyze_audio_emotion(audio_file: str) -> Dict:
    """
    å¿«é€ŸéŸ³é¢‘æƒ…ç»ªåˆ†æ
    
    Args:
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        æƒ…ç»ªåˆ†æç»“æœ
    """
    api = get_emotion_api()
    return api.analyze_audio_file(audio_file)


def analyze_image_emotion(image_file: str) -> Dict:
    """
    å¿«é€Ÿå›¾åƒæƒ…ç»ªåˆ†æ
    
    Args:
        image_file: å›¾åƒæ–‡ä»¶è·¯å¾„
        
    Returns:
        æƒ…ç»ªåˆ†æç»“æœ
    """
    api = get_emotion_api()
    return api.analyze_image_file(image_file)


def check_api_status() -> Dict:
    """æ£€æŸ¥APIçŠ¶æ€"""
    api = get_emotion_api()
    return api.get_status()


def demo_quick_api():
    """æ¼”ç¤ºå¿«é€ŸAPI"""
    print("=" * 50)
    print("å¿«é€Ÿæƒ…ç»ªè¯†åˆ«APIæ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥çŠ¶æ€
    status = check_api_status()
    print(f"APIçŠ¶æ€: {'âœ… å°±ç»ª' if status['ready'] else 'âŒ æœªå°±ç»ª'}")
    
    if not status['ready']:
        print("APIæœªå‡†å¤‡å°±ç»ªï¼Œè¯·æ£€æŸ¥æ¨¡å—é…ç½®")
        return
    
    print("\né€‰æ‹©åŠŸèƒ½:")
    print("1. å¿«é€Ÿæƒ…ç»ªæ£€æµ‹ï¼ˆ5ç§’å½•åˆ¶ï¼‰")
    print("2. æ–‡æœ¬æƒ…ç»ªåˆ†æ")
    print("3. éŸ³é¢‘æ–‡ä»¶åˆ†æ")
    print("4. å›¾åƒæ–‡ä»¶åˆ†æ")
    
    try:
        choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            print("\n=== å¿«é€Ÿæƒ…ç»ªæ£€æµ‹ ===")
            result = quick_emotion_check(5)
            
            if result['success']:
                print(f"âœ… {result['summary']}")
                print(f"æƒ…ç»ªåˆ†å¸ƒ: {result['emotion_distribution']}")
                print(f"æ£€æµ‹æ¬¡æ•°: {result['total_detections']}")
            else:
                print(f"âŒ æ£€æµ‹å¤±è´¥: {result['error']}")
        
        elif choice == '2':
            text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ")
            result = analyze_text_emotion(text)
            
            if result['success']:
                print(f"âœ… æ–‡æœ¬æƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        
        elif choice == '3':
            audio_file = input("è¯·è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„: ")
            result = analyze_audio_emotion(audio_file)
            
            if result['success']:
                print(f"âœ… éŸ³é¢‘æƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        
        elif choice == '4':
            image_file = input("è¯·è¾“å…¥å›¾åƒæ–‡ä»¶è·¯å¾„: ")
            result = analyze_image_emotion(image_file)
            
            if result['success']:
                print(f"âœ… å›¾åƒæƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        
        else:
            print("æ— æ•ˆé€‰æ‹©")
    
    except KeyboardInterrupt:
        print("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    print("\næ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_quick_api() 