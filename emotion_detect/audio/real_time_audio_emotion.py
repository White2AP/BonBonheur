#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«æ¨¡å—
æ”¯æŒéº¦å…‹é£å®æ—¶å½•éŸ³å’Œæƒ…ç»ªè¯†åˆ«
"""

import numpy as np
import pyaudio
import wave
import threading
import time
import queue
from audio_emotion_recognition import AudioEmotionRecognizer
import tempfile
import os

class RealTimeAudioEmotionRecognizer:
    """å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«å™¨"""

    
    def __init__(self, model_path="models/audio_emotion_model.pkl"):
        """
        åˆå§‹åŒ–å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        """
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        self.recognizer = AudioEmotionRecognizer()
        try:
            self.recognizer.load_model(model_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except FileNotFoundError:
            print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            raise
        
        # éŸ³é¢‘å‚æ•°
        self.sample_rate = 22050
        self.chunk_size = 1024
        self.channels = 1
        self.format = pyaudio.paInt16
        self.record_duration = 3  # å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
        
        # PyAudioå¯¹è±¡
        self.audio = pyaudio.PyAudio()
        
        # æ§åˆ¶å˜é‡
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.result_queue = queue.Queue()
        
        # å›è°ƒå‡½æ•°
        self.emotion_callback = None
    
    def set_emotion_callback(self, callback):
        """
        è®¾ç½®æƒ…ç»ªè¯†åˆ«ç»“æœå›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶è¯†åˆ«ç»“æœä½œä¸ºå‚æ•°
        """
        self.emotion_callback = callback
    
    def record_audio_chunk(self):
        """å½•åˆ¶ä¸€æ®µéŸ³é¢‘"""
        try:
            stream = self.audio.open(
                format=self.format,
                channels=self.channels,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size
            )
            
            frames = []
            for _ in range(int(self.sample_rate / self.chunk_size * self.record_duration)):
                if not self.is_recording:
                    break
                data = stream.read(self.chunk_size)
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            if frames:
                # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
                temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                
                wf = wave.open(temp_file.name, 'wb')
                wf.setnchannels(self.channels)
                wf.setsampwidth(self.audio.get_sample_size(self.format))
                wf.setframerate(self.sample_rate)
                wf.writeframes(b''.join(frames))
                wf.close()
                
                return temp_file.name
            
        except Exception as e:
            print(f"å½•éŸ³é”™è¯¯: {e}")
        
        return None
    
    def process_audio_worker(self):
        """éŸ³é¢‘å¤„ç†å·¥ä½œçº¿ç¨‹"""
        while self.is_recording:
            try:
                # å½•åˆ¶éŸ³é¢‘
                audio_file = self.record_audio_chunk()
                
                if audio_file:
                    # è¯†åˆ«æƒ…ç»ª
                    result = self.recognizer.predict_emotion(audio_file)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(audio_file)
                    
                    if result:
                        # æ·»åŠ æ—¶é—´æˆ³
                        result['timestamp'] = time.time()
                        
                        # æ”¾å…¥ç»“æœé˜Ÿåˆ—
                        self.result_queue.put(result)
                        
                        # è°ƒç”¨å›è°ƒå‡½æ•°
                        if self.emotion_callback:
                            self.emotion_callback(result)
                
            except Exception as e:
                print(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(0.1)
    
    def start_recognition(self):
        """å¼€å§‹å®æ—¶æƒ…ç»ªè¯†åˆ«"""
        if self.is_recording:
            print("âš ï¸ å·²ç»åœ¨å½•éŸ³ä¸­")
            return
        
        print("ğŸ¤ å¼€å§‹å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«...")
        self.is_recording = True
        
        # å¯åŠ¨éŸ³é¢‘å¤„ç†çº¿ç¨‹
        self.process_thread = threading.Thread(target=self.process_audio_worker)
        self.process_thread.daemon = True
        self.process_thread.start()
    
    def stop_recognition(self):
        """åœæ­¢å®æ—¶æƒ…ç»ªè¯†åˆ«"""
        if not self.is_recording:
            print("âš ï¸ æ²¡æœ‰åœ¨å½•éŸ³")
            return
        
        print("ğŸ›‘ åœæ­¢å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«")
        self.is_recording = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if hasattr(self, 'process_thread'):
            self.process_thread.join(timeout=2)
    
    def get_latest_result(self):
        """è·å–æœ€æ–°çš„è¯†åˆ«ç»“æœ"""
        try:
            return self.result_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_all_results(self):
        """è·å–æ‰€æœ‰è¯†åˆ«ç»“æœ"""
        results = []
        while True:
            try:
                result = self.result_queue.get_nowait()
                results.append(result)
            except queue.Empty:
                break
        return results
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'audio'):
            self.audio.terminate()

class EmotionMonitor:
    """æƒ…ç»ªç›‘æ§å™¨ - ç”¨äºç»Ÿè®¡å’Œåˆ†ææƒ…ç»ªå˜åŒ–"""
    
    def __init__(self, window_size=10):
        """
        åˆå§‹åŒ–æƒ…ç»ªç›‘æ§å™¨
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        """
        self.window_size = window_size
        self.emotion_history = []
        self.confidence_history = []
        self.timestamp_history = []
    
    def add_result(self, result):
        """æ·»åŠ è¯†åˆ«ç»“æœ"""
        self.emotion_history.append(result['emotion'])
        self.confidence_history.append(result['confidence'])
        self.timestamp_history.append(result['timestamp'])
        
        # ä¿æŒçª—å£å¤§å°
        if len(self.emotion_history) > self.window_size:
            self.emotion_history.pop(0)
            self.confidence_history.pop(0)
            self.timestamp_history.pop(0)
    
    def get_dominant_emotion(self):
        """è·å–ä¸»å¯¼æƒ…ç»ª"""
        if not self.emotion_history:
            return None
        
        # ç»Ÿè®¡æƒ…ç»ªé¢‘æ¬¡
        emotion_counts = {}
        for emotion in self.emotion_history:
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        # è¿”å›æœ€é¢‘ç¹çš„æƒ…ç»ª
        return max(emotion_counts, key=emotion_counts.get)
    
    def get_average_confidence(self):
        """è·å–å¹³å‡ç½®ä¿¡åº¦"""
        if not self.confidence_history:
            return 0.0
        return np.mean(self.confidence_history)
    
    def get_emotion_stability(self):
        """è·å–æƒ…ç»ªç¨³å®šæ€§ï¼ˆå˜åŒ–é¢‘ç‡ï¼‰"""
        if len(self.emotion_history) < 2:
            return 1.0
        
        changes = 0
        for i in range(1, len(self.emotion_history)):
            if self.emotion_history[i] != self.emotion_history[i-1]:
                changes += 1
        
        return 1.0 - (changes / (len(self.emotion_history) - 1))
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.emotion_history:
            return None
        
        return {
            'dominant_emotion': self.get_dominant_emotion(),
            'average_confidence': self.get_average_confidence(),
            'emotion_stability': self.get_emotion_stability(),
            'total_samples': len(self.emotion_history),
            'emotion_distribution': {
                emotion: self.emotion_history.count(emotion) / len(self.emotion_history)
                for emotion in set(self.emotion_history)
            }
        }

def demo_real_time_recognition():
    """æ¼”ç¤ºå®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«"""
    print("ğŸµ å®æ—¶éŸ³é¢‘æƒ…ç»ªè¯†åˆ«æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå®æ—¶è¯†åˆ«å™¨
        recognizer = RealTimeAudioEmotionRecognizer()
        
        # åˆ›å»ºæƒ…ç»ªç›‘æ§å™¨
        monitor = EmotionMonitor()
        
        # è®¾ç½®å›è°ƒå‡½æ•°
        def emotion_callback(result):
            monitor.add_result(result)
            print(f"ğŸ­ æ£€æµ‹åˆ°æƒ…ç»ª: {result['emotion']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            
            # æ¯5ä¸ªç»“æœæ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
            if len(monitor.emotion_history) % 5 == 0:
                stats = monitor.get_statistics()
                if stats:
                    print(f"ğŸ“Š ä¸»å¯¼æƒ…ç»ª: {stats['dominant_emotion']}")
                    print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {stats['average_confidence']:.3f}")
                    print(f"ğŸ“Š æƒ…ç»ªç¨³å®šæ€§: {stats['emotion_stability']:.3f}")
                    print("-" * 30)
        
        recognizer.set_emotion_callback(emotion_callback)
        
        # å¼€å§‹è¯†åˆ«
        recognizer.start_recognition()
        
        print("ğŸ¤ è¯·å¯¹ç€éº¦å…‹é£è¯´è¯...")
        print("æŒ‰ Enter é”®åœæ­¢è¯†åˆ«")
        
        # ç­‰å¾…ç”¨æˆ·è¾“å…¥
        input()
        
        # åœæ­¢è¯†åˆ«
        recognizer.stop_recognition()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        final_stats = monitor.get_statistics()
        if final_stats:
            print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
            print(f"  ä¸»å¯¼æƒ…ç»ª: {final_stats['dominant_emotion']}")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {final_stats['average_confidence']:.3f}")
            print(f"  æƒ…ç»ªç¨³å®šæ€§: {final_stats['emotion_stability']:.3f}")
            print(f"  æ€»æ ·æœ¬æ•°: {final_stats['total_samples']}")
            print("  æƒ…ç»ªåˆ†å¸ƒ:")
            for emotion, ratio in final_stats['emotion_distribution'].items():
                print(f"    {emotion}: {ratio:.2%}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    demo_real_time_recognition() 