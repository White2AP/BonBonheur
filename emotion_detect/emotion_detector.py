#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†è§‰æƒ…ç»ªè¯†åˆ«æ¨¡å—
åŸºäºCNNæ¨¡å‹è¿›è¡Œäººè„¸è¡¨æƒ…è¯†åˆ«
æ”¯æŒçš„æƒ…ç»ªï¼šangry, detester, peur, content, triste, surprise, nature
"""

import cv2
import numpy as np
import os
import logging
from typing import List, Dict, Optional, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VisionEmotionDetector:
    """è§†è§‰æƒ…ç»ªè¯†åˆ«å™¨"""
    
    def __init__(self, model_path: str = "emotion_detection_cnn.h5"):
        """
        åˆå§‹åŒ–è§†è§‰æƒ…ç»ªè¯†åˆ«å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.face_cascade = None
        
        # æƒ…ç»ªæ ‡ç­¾ï¼ˆä¸æ¨¡å‹è®­ç»ƒæ—¶çš„é¡ºåºä¸€è‡´ï¼‰
        self.emotions = ["angry", "detester", "peur", "content", "triste", "surprise", "nature"]
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        self._init_face_detector()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _init_face_detector(self):
        """åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨"""
        try:
            self.face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            logger.info("âœ… äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.face_cascade = None
    
    def _load_model(self):
        """åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
        try:
            if not os.path.exists(self.model_path):
                logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                return
            
            from keras.models import load_model
            self.model = load_model(self.model_path)
            logger.info(f"âœ… æƒ…ç»ªè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            
        except ImportError as e:
            logger.error(f"âŒ æ— æ³•å¯¼å…¥Keras: {e}")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ£€æµ‹å™¨æ˜¯å¦å‡†å¤‡å°±ç»ª"""
        return self.model is not None and self.face_cascade is not None
    
    def preprocess_face(self, face_roi: np.ndarray, target_size: Tuple[int, int] = (128, 128)) -> np.ndarray:
        """
        é¢„å¤„ç†äººè„¸å›¾åƒ
        
        Args:
            face_roi: äººè„¸åŒºåŸŸå›¾åƒ
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            é¢„å¤„ç†åçš„å›¾åƒå¼ é‡
        """
        try:
            # è°ƒæ•´å¤§å°
            resized = cv2.resize(face_roi, target_size)
            
            # å½’ä¸€åŒ–
            normalized = resized.astype("float32") / 255.0
            
            # æ·»åŠ æ‰¹æ¬¡å’Œé€šé“ç»´åº¦
            input_tensor = np.expand_dims(normalized, axis=(0, -1))
            
            return input_tensor
            
        except Exception as e:
            logger.error(f"äººè„¸é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def detect_faces(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„äººè„¸
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            äººè„¸è¾¹ç•Œæ¡†åˆ—è¡¨ [(x, y, w, h), ...]
        """
        if self.face_cascade is None:
            return []
        
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image
            
            # æ£€æµ‹äººè„¸
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.3, 
                minNeighbors=5,
                minSize=(30, 30)
            )
            
            return faces.tolist() if len(faces) > 0 else []
            
        except Exception as e:
            logger.error(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def predict_emotion_from_face(self, face_roi: np.ndarray) -> Dict:
        """
        ä»äººè„¸åŒºåŸŸé¢„æµ‹æƒ…ç»ª
        
        Args:
            face_roi: äººè„¸åŒºåŸŸå›¾åƒï¼ˆç°åº¦å›¾ï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        if self.model is None:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'error': 'æ¨¡å‹æœªåŠ è½½'
            }
        
        try:
            # é¢„å¤„ç†
            input_tensor = self.preprocess_face(face_roi)
            if input_tensor is None:
                return {
                    'emotion': 'nature',
                    'confidence': 0.0,
                    'error': 'é¢„å¤„ç†å¤±è´¥'
                }
            
            # é¢„æµ‹
            predictions = self.model.predict(input_tensor, verbose=0)[0]
            
            # è·å–æœ€é«˜æ¦‚ç‡çš„æƒ…ç»ª
            emotion_idx = np.argmax(predictions)
            emotion = self.emotions[emotion_idx]
            confidence = float(predictions[emotion_idx])
            
            return {
                'emotion': emotion,
                'confidence': confidence,
                'probabilities': {
                    self.emotions[i]: float(prob) 
                    for i, prob in enumerate(predictions)
                },
                'method': 'vision'
            }
            
        except Exception as e:
            logger.error(f"æƒ…ç»ªé¢„æµ‹å¤±è´¥: {e}")
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'
            }
    
    def analyze_image(self, image: np.ndarray) -> Dict:
        """
        åˆ†æå•å¼ å›¾åƒçš„æƒ…ç»ª
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.is_ready():
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'faces_detected': 0,
                'error': 'æ£€æµ‹å™¨æœªå‡†å¤‡å°±ç»ª'
            }
        
        # æ£€æµ‹äººè„¸
        faces = self.detect_faces(image)
        
        if not faces:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'faces_detected': 0,
                'error': 'æœªæ£€æµ‹åˆ°äººè„¸'
            }
        
        # åˆ†ææ¯ä¸ªäººè„¸çš„æƒ…ç»ª
        face_emotions = []
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        for (x, y, w, h) in faces:
            # æå–äººè„¸åŒºåŸŸ
            face_roi = gray[y:y + h, x:x + w]
            
            # é¢„æµ‹æƒ…ç»ª
            emotion_result = self.predict_emotion_from_face(face_roi)
            emotion_result['bbox'] = (x, y, w, h)
            face_emotions.append(emotion_result)
        
        # å¦‚æœæœ‰å¤šä¸ªäººè„¸ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
        if face_emotions:
            best_result = max(face_emotions, key=lambda x: x.get('confidence', 0))
            
            return {
                'emotion': best_result['emotion'],
                'confidence': best_result['confidence'],
                'faces_detected': len(faces),
                'all_faces': face_emotions,
                'method': 'vision'
            }
        else:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'faces_detected': len(faces),
                'error': 'äººè„¸æƒ…ç»ªåˆ†æå¤±è´¥'
            }
    
    def analyze_video_frames(self, frames: List[np.ndarray]) -> Dict:
        """
        åˆ†æè§†é¢‘å¸§åºåˆ—çš„æƒ…ç»ª
        
        Args:
            frames: è§†é¢‘å¸§åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœ
        """
        if not frames:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'total_frames': 0,
                'valid_detections': 0,
                'error': 'æ²¡æœ‰è¾“å…¥å¸§'
            }
        
        emotion_counts = {}
        confidence_scores = []
        valid_detections = 0
        
        for frame in frames:
            result = self.analyze_image(frame)
            
            if result.get('faces_detected', 0) > 0 and 'error' not in result:
                emotion = result['emotion']
                confidence = result['confidence']
                
                # ç»Ÿè®¡æƒ…ç»ª
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
                confidence_scores.append(confidence)
                valid_detections += 1
        
        if valid_detections > 0:
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„æƒ…ç»ª
            dominant_emotion = max(emotion_counts, key=emotion_counts.get)
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            
            return {
                'emotion': dominant_emotion,
                'confidence': avg_confidence,
                'total_frames': len(frames),
                'valid_detections': valid_detections,
                'emotion_distribution': emotion_counts,
                'method': 'vision'
            }
        else:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'total_frames': len(frames),
                'valid_detections': 0,
                'error': 'æ‰€æœ‰å¸§éƒ½æœªæ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸'
            }
    
    def analyze_video_file(self, video_path: str, max_frames: int = 100) -> Dict:
        """
        åˆ†æè§†é¢‘æ–‡ä»¶çš„æƒ…ç»ª
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            max_frames: æœ€å¤§åˆ†æå¸§æ•°
            
        Returns:
            åˆ†æç»“æœ
        """
        if not os.path.exists(video_path):
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'error': f'è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}'
            }
        
        try:
            cap = cv2.VideoCapture(video_path)
            frames = []
            
            frame_count = 0
            while frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
                frame_count += 1
            
            cap.release()
            
            if not frames:
                return {
                    'emotion': 'nature',
                    'confidence': 0.0,
                    'error': 'æ— æ³•è¯»å–è§†é¢‘å¸§'
                }
            
            return self.analyze_video_frames(frames)
            
        except Exception as e:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'error': f'è§†é¢‘åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def real_time_detection(self, duration: int = 5) -> Dict:
        """
        å®æ—¶æƒ…ç»ªæ£€æµ‹
        
        Args:
            duration: æ£€æµ‹æ—¶é•¿ï¼ˆç§’ï¼‰
            
        Returns:
            æ£€æµ‹ç»“æœ
        """
        try:
            cap = cv2.VideoCapture(0)
            if not cap.isOpened():
                return {
                    'emotion': 'nature',
                    'confidence': 0.0,
                    'error': 'æ— æ³•æ‰“å¼€æ‘„åƒå¤´'
                }
            
            frames = []
            import time
            start_time = time.time()
            
            while time.time() - start_time < duration:
                ret, frame = cap.read()
                if ret:
                    frames.append(frame.copy())
                time.sleep(0.1)  # æ§åˆ¶å¸§ç‡
            
            cap.release()
            
            if not frames:
                return {
                    'emotion': 'nature',
                    'confidence': 0.0,
                    'error': 'æœªæ•è·åˆ°è§†é¢‘å¸§'
                }
            
            return self.analyze_video_frames(frames)
            
        except Exception as e:
            return {
                'emotion': 'nature',
                'confidence': 0.0,
                'error': f'å®æ—¶æ£€æµ‹å¤±è´¥: {str(e)}'
            }


def demo_vision_detection():
    """æ¼”ç¤ºè§†è§‰æƒ…ç»ªæ£€æµ‹åŠŸèƒ½"""
    print("ğŸ‘ï¸ è§†è§‰æƒ…ç»ªè¯†åˆ«æ¼”ç¤º")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = VisionEmotionDetector()
    
    if not detector.is_ready():
        print("âŒ æ£€æµ‹å™¨æœªå‡†å¤‡å°±ç»ªï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–")
        return
    
    print("âœ… æ£€æµ‹å™¨å‡†å¤‡å°±ç»ª")
    print(f"æ”¯æŒçš„æƒ…ç»ª: {detector.emotions}")
    
    print("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å®æ—¶æ£€æµ‹ï¼ˆéœ€è¦æ‘„åƒå¤´ï¼‰")
    print("2. åˆ†æè§†é¢‘æ–‡ä»¶")
    print("3. é€€å‡º")
    
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == '1':
            print("\nå‡†å¤‡å¼€å§‹å®æ—¶æ£€æµ‹ï¼Œè¯·é¢å¯¹æ‘„åƒå¤´...")
            input("æŒ‰å›è½¦é”®å¼€å§‹æ£€æµ‹ï¼ˆ5ç§’ï¼‰...")
            
            result = detector.real_time_detection(duration=5)
            
            if 'error' not in result:
                print(f"\nğŸ¯ æ£€æµ‹ç»“æœ:")
                print(f"   æƒ…ç»ª: {result['emotion']}")
                print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                print(f"   æœ‰æ•ˆæ£€æµ‹: {result['valid_detections']}/{result['total_frames']}")
                if result.get('emotion_distribution'):
                    print(f"   æƒ…ç»ªåˆ†å¸ƒ: {result['emotion_distribution']}")
            else:
                print(f"âŒ æ£€æµ‹å¤±è´¥: {result['error']}")
        
        elif choice == '2':
            video_path = input("è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ").strip()
            if video_path:
                result = detector.analyze_video_file(video_path)
                
                if 'error' not in result:
                    print(f"\nğŸ¯ åˆ†æç»“æœ:")
                    print(f"   æƒ…ç»ª: {result['emotion']}")
                    print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                    print(f"   æœ‰æ•ˆæ£€æµ‹: {result['valid_detections']}/{result['total_frames']}")
                else:
                    print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        
        elif choice == '3':
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    demo_vision_detection() 