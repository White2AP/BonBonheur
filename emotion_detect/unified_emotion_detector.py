#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æƒ…ç»ªè¯†åˆ«æ¥å£
æ•´åˆè¯­éŸ³ã€è¯­è¨€å’Œè¡¨æƒ…ä¸‰ä¸ªæ¨¡å—ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«æœåŠ¡
æŠ€æœ¯è·¯çº¿ï¼šè¯·æ±‚ -> å¼€å§‹å½•åˆ¶éŸ³è§†é¢‘ -> è§£æ -> åé¦ˆæƒ…ç»ª
"""

import cv2
import numpy as np
import threading
import time
import queue
import tempfile
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Callable, Tuple
import logging

# å¯¼å…¥å„ä¸ªæ¨¡å—
from nlp.emotion_pipeline import EmotionAnalysisPipeline
from audio.real_time_audio_emotion import RealTimeAudioEmotionRecognizer
from vision.emotion_detector import VisionEmotionDetector

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UnifiedEmotionDetector:
    """
    ç»Ÿä¸€æƒ…ç»ªè¯†åˆ«å™¨
    æ•´åˆè¯­éŸ³ã€è¯­è¨€å’Œè¡¨æƒ…ä¸‰ä¸ªæ¨¡å—çš„å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«
    """
    
    def __init__(self, 
                 audio_model_path: str = "audio/models/audio_emotion_model.pkl",
                 vision_model_path: str = "vision/emotion_detection_cnn.h5",
                 enable_audio: bool = True,
                 enable_vision: bool = True,
                 enable_nlp: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æƒ…ç»ªè¯†åˆ«å™¨
        
        Args:
            audio_model_path: éŸ³é¢‘æ¨¡å‹è·¯å¾„
            vision_model_path: è§†è§‰æ¨¡å‹è·¯å¾„
            enable_audio: æ˜¯å¦å¯ç”¨éŸ³é¢‘è¯†åˆ«
            enable_vision: æ˜¯å¦å¯ç”¨è§†è§‰è¯†åˆ«
            enable_nlp: æ˜¯å¦å¯ç”¨è¯­è¨€è¯†åˆ«
        """
        self.enable_audio = enable_audio
        self.enable_vision = enable_vision
        self.enable_nlp = enable_nlp
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.audio_detector = None
        self.vision_detector = None
        self.nlp_pipeline = None
        
        self._init_modules(audio_model_path, vision_model_path)
        
        # å½•åˆ¶æ§åˆ¶
        self.is_recording = False
        self.recording_thread = None
        self.video_capture = None
        
        # ç»“æœé˜Ÿåˆ—
        self.audio_results = queue.Queue()
        self.vision_results = queue.Queue()
        self.nlp_results = queue.Queue()
        self.unified_results = queue.Queue()
        
        # å›è°ƒå‡½æ•°
        self.result_callback = None
        
        # æƒ…ç»ªæ˜ å°„ï¼ˆç»Ÿä¸€ä¸åŒæ¨¡å—çš„æƒ…ç»ªæ ‡ç­¾ï¼‰
        self.emotion_mapping = {
            # éŸ³é¢‘æ¨¡å—æ˜ å°„
            'happy': 'å¿«ä¹',
            'sad': 'æ‚²ä¼¤',
            'angry': 'æ„¤æ€’',
            'fear': 'ææƒ§',
            'surprise': 'æƒŠè®¶',
            'neutral': 'ä¸­æ€§',
            
            # è§†è§‰æ¨¡å—æ˜ å°„
            'content': 'å¿«ä¹',
            'triste': 'æ‚²ä¼¤',
            'peur': 'ææƒ§',
            'detester': 'æ„¤æ€’',
            'nature': 'ä¸­æ€§',
            
            # NLPæ¨¡å—æ˜ å°„ï¼ˆå·²ç»æ˜¯ä¸­æ–‡ï¼‰
            'ç§¯æ': 'ç§¯æ',
            'æ¶ˆæ': 'æ¶ˆæ',
            'æ„¤æ€’': 'æ„¤æ€’',
            'æ‚²ä¼¤': 'æ‚²ä¼¤',
            'å¿«ä¹': 'å¿«ä¹',
            'ææƒ§': 'ææƒ§',
            'æƒŠè®¶': 'æƒŠè®¶',
            'ä¸­æ€§': 'ä¸­æ€§'
        }
    
    def _init_modules(self, audio_model_path: str, vision_model_path: str):
        """åˆå§‹åŒ–å„ä¸ªè¯†åˆ«æ¨¡å—"""
        
        # åˆå§‹åŒ–éŸ³é¢‘æ¨¡å—
        if self.enable_audio:
            try:
                self.audio_detector = RealTimeAudioEmotionRecognizer(audio_model_path)
                logger.info("âœ… éŸ³é¢‘æƒ…ç»ªè¯†åˆ«æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ éŸ³é¢‘æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_audio = False
        
        # åˆå§‹åŒ–è§†è§‰æ¨¡å—
        if self.enable_vision:
            try:
                self.vision_detector = VisionEmotionDetector(vision_model_path)
                if not self.vision_detector.is_ready():
                    raise Exception("è§†è§‰æ¨¡å—æœªå‡†å¤‡å°±ç»ª")
                logger.info("âœ… è§†è§‰æƒ…ç»ªè¯†åˆ«æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ è§†è§‰æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_vision = False
        
        # åˆå§‹åŒ–NLPæ¨¡å—
        if self.enable_nlp:
            try:
                self.nlp_pipeline = EmotionAnalysisPipeline()
                logger.info("âœ… è¯­è¨€æƒ…ç»ªè¯†åˆ«æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ NLPæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_nlp = False
    
    def set_result_callback(self, callback: Callable[[Dict], None]):
        """
        è®¾ç½®ç»“æœå›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ç»Ÿä¸€çš„æƒ…ç»ªè¯†åˆ«ç»“æœ
        """
        self.result_callback = callback
    
    def start_recording(self, duration: int = 10):
        """
        å¼€å§‹å½•åˆ¶éŸ³è§†é¢‘å¹¶è¿›è¡Œæƒ…ç»ªè¯†åˆ«
        
        Args:
            duration: å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæŒç»­å½•åˆ¶ç›´åˆ°æ‰‹åŠ¨åœæ­¢
        """
        if self.is_recording:
            logger.warning("âš ï¸ å·²ç»åœ¨å½•åˆ¶ä¸­")
            return False
        
        logger.info(f"ğŸ¬ å¼€å§‹å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«å½•åˆ¶ï¼Œæ—¶é•¿: {duration}ç§’" if duration > 0 else "ğŸ¬ å¼€å§‹æŒç»­å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«å½•åˆ¶")
        
        self.is_recording = True
        
        # å¯åŠ¨å½•åˆ¶çº¿ç¨‹
        self.recording_thread = threading.Thread(
            target=self._recording_worker, 
            args=(duration,)
        )
        self.recording_thread.daemon = True
        self.recording_thread.start()
        
        return True
    
    def stop_recording(self):
        """åœæ­¢å½•åˆ¶"""
        if not self.is_recording:
            logger.warning("âš ï¸ æ²¡æœ‰åœ¨å½•åˆ¶")
            return False
        
        logger.info("ğŸ›‘ åœæ­¢å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«å½•åˆ¶")
        self.is_recording = False
        
        # åœæ­¢å„ä¸ªæ¨¡å—
        if self.enable_audio and self.audio_detector:
            self.audio_detector.stop_recognition()
        
        if self.video_capture:
            self.video_capture.release()
            self.video_capture = None
        
        # ç­‰å¾…å½•åˆ¶çº¿ç¨‹ç»“æŸ
        if self.recording_thread:
            self.recording_thread.join(timeout=3)
        
        return True
    
    def _recording_worker(self, duration: int):
        """å½•åˆ¶å·¥ä½œçº¿ç¨‹"""
        start_time = time.time()
        
        # å¯åŠ¨éŸ³é¢‘è¯†åˆ«
        if self.enable_audio and self.audio_detector:
            self.audio_detector.set_emotion_callback(self._audio_callback)
            self.audio_detector.start_recognition()
        
        # å¯åŠ¨è§†é¢‘æ•è·
        if self.enable_vision:
            try:
                self.video_capture = cv2.VideoCapture(0)
                if not self.video_capture.isOpened():
                    logger.error("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                    self.enable_vision = False
            except Exception as e:
                logger.error(f"âŒ æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_vision = False
        
        # ä¸»å½•åˆ¶å¾ªç¯
        frame_count = 0
        last_vision_time = 0
        vision_interval = 1.0  # æ¯ç§’åˆ†æä¸€æ¬¡è§†è§‰æƒ…ç»ª
        
        while self.is_recording:
            current_time = time.time()
            
            # æ£€æŸ¥å½•åˆ¶æ—¶é•¿
            if duration > 0 and (current_time - start_time) >= duration:
                break
            
            # å¤„ç†è§†é¢‘å¸§
            if self.enable_vision and self.video_capture:
                ret, frame = self.video_capture.read()
                if ret:
                    frame_count += 1
                    
                    # å®šæœŸè¿›è¡Œè§†è§‰æƒ…ç»ªåˆ†æ
                    if current_time - last_vision_time >= vision_interval:
                        self._analyze_vision_frame(frame, current_time)
                        last_vision_time = current_time
            
            # èåˆå¤šæ¨¡æ€ç»“æœ
            self._fuse_multimodal_results()
            
            time.sleep(0.1)  # é¿å…è¿‡åº¦å ç”¨CPU
        
        # æ¸…ç†èµ„æº
        self.is_recording = False
        if self.video_capture:
            self.video_capture.release()
            self.video_capture = None
    
    def _audio_callback(self, audio_result: Dict):
        """éŸ³é¢‘è¯†åˆ«ç»“æœå›è°ƒ"""
        try:
            # æ ‡å‡†åŒ–éŸ³é¢‘ç»“æœ
            standardized_result = {
                'modality': 'audio',
                'timestamp': audio_result.get('timestamp', time.time()),
                'emotion': self.emotion_mapping.get(audio_result.get('emotion', 'neutral'), 'ä¸­æ€§'),
                'confidence': audio_result.get('confidence', 0.0),
                'raw_result': audio_result
            }
            
            self.audio_results.put(standardized_result)
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘ç»“æœå¤„ç†å¤±è´¥: {e}")
    
    def _analyze_vision_frame(self, frame: np.ndarray, timestamp: float):
        """åˆ†æè§†é¢‘å¸§"""
        try:
            if self.vision_detector:
                result = self.vision_detector.analyze_image(frame)
                
                if result.get('faces_detected', 0) > 0:
                    # å–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸ç»“æœ
                    face_result = result['faces'][0] if result.get('faces') else {}
                    
                    standardized_result = {
                        'modality': 'vision',
                        'timestamp': timestamp,
                        'emotion': self.emotion_mapping.get(face_result.get('emotion', 'nature'), 'ä¸­æ€§'),
                        'confidence': face_result.get('confidence', 0.0),
                        'raw_result': result
                    }
                    
                    self.vision_results.put(standardized_result)
        
        except Exception as e:
            logger.error(f"è§†è§‰åˆ†æå¤±è´¥: {e}")
    
    def _fuse_multimodal_results(self):
        """èåˆå¤šæ¨¡æ€ç»“æœ"""
        try:
            # æ”¶é›†æœ€è¿‘çš„ç»“æœ
            recent_results = {
                'audio': [],
                'vision': [],
                'nlp': []
            }
            
            current_time = time.time()
            time_window = 2.0  # 2ç§’æ—¶é—´çª—å£
            
            # æ”¶é›†éŸ³é¢‘ç»“æœ
            while not self.audio_results.empty():
                try:
                    result = self.audio_results.get_nowait()
                    if current_time - result['timestamp'] <= time_window:
                        recent_results['audio'].append(result)
                except queue.Empty:
                    break
            
            # æ”¶é›†è§†è§‰ç»“æœ
            while not self.vision_results.empty():
                try:
                    result = self.vision_results.get_nowait()
                    if current_time - result['timestamp'] <= time_window:
                        recent_results['vision'].append(result)
                except queue.Empty:
                    break
            
            # æ”¶é›†NLPç»“æœ
            while not self.nlp_results.empty():
                try:
                    result = self.nlp_results.get_nowait()
                    if current_time - result['timestamp'] <= time_window:
                        recent_results['nlp'].append(result)
                except queue.Empty:
                    break
            
            # å¦‚æœæœ‰ç»“æœï¼Œè¿›è¡Œèåˆ
            if any(recent_results.values()):
                fused_result = self._compute_fused_emotion(recent_results)
                
                if fused_result:
                    self.unified_results.put(fused_result)
                    
                    # è°ƒç”¨å›è°ƒå‡½æ•°
                    if self.result_callback:
                        self.result_callback(fused_result)
        
        except Exception as e:
            logger.error(f"ç»“æœèåˆå¤±è´¥: {e}")
    
    def _compute_fused_emotion(self, recent_results: Dict) -> Optional[Dict]:
        """è®¡ç®—èåˆåçš„æƒ…ç»ª"""
        try:
            # æƒ…ç»ªæŠ•ç¥¨ç»Ÿè®¡
            emotion_votes = {}
            total_confidence = 0.0
            modality_count = 0
            
            # æƒé‡è®¾ç½®
            modality_weights = {
                'audio': 0.4,
                'vision': 0.4,
                'nlp': 0.2
            }
            
            modality_results = {}
            
            for modality, results in recent_results.items():
                if results:
                    # å–æœ€æ–°çš„ç»“æœ
                    latest_result = max(results, key=lambda x: x['timestamp'])
                    emotion = latest_result['emotion']
                    confidence = latest_result['confidence']
                    
                    # åŠ æƒæŠ•ç¥¨
                    weight = modality_weights.get(modality, 0.3)
                    weighted_confidence = confidence * weight
                    
                    if emotion in emotion_votes:
                        emotion_votes[emotion] += weighted_confidence
                    else:
                        emotion_votes[emotion] = weighted_confidence
                    
                    total_confidence += weighted_confidence
                    modality_count += 1
                    
                    modality_results[modality] = {
                        'emotion': emotion,
                        'confidence': confidence,
                        'timestamp': latest_result['timestamp']
                    }
            
            if not emotion_votes:
                return None
            
            # æ‰¾åˆ°å¾—ç¥¨æœ€é«˜çš„æƒ…ç»ª
            final_emotion = max(emotion_votes, key=emotion_votes.get)
            final_confidence = emotion_votes[final_emotion]
            
            # å½’ä¸€åŒ–ç½®ä¿¡åº¦
            if total_confidence > 0:
                final_confidence = min(final_confidence / total_confidence * modality_count, 1.0)
            
            return {
                'timestamp': time.time(),
                'final_emotion': final_emotion,
                'confidence': final_confidence,
                'modality_results': modality_results,
                'emotion_votes': emotion_votes,
                'fusion_method': 'weighted_voting'
            }
        
        except Exception as e:
            logger.error(f"æƒ…ç»ªèåˆè®¡ç®—å¤±è´¥: {e}")
            return None
    
    def analyze_text(self, text: str) -> Dict:
        """
        åˆ†ææ–‡æœ¬æƒ…ç»ªï¼ˆå•ç‹¬è°ƒç”¨NLPæ¨¡å—ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.enable_nlp or not self.nlp_pipeline:
            return {'error': 'NLPæ¨¡å—æœªå¯ç”¨æˆ–æœªåˆå§‹åŒ–'}
        
        try:
            # ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æ
            normalized_text = self.nlp_pipeline.emotion_classifier.normalize_text(text)
            result = self.nlp_pipeline._rule_based_emotion_analysis(normalized_text)
            
            return {
                'modality': 'nlp',
                'timestamp': time.time(),
                'emotion': result['predicted_emotion'],
                'confidence': result['confidence'],
                'raw_result': result
            }
        
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†æå¤±è´¥: {e}")
            return {'error': f'æ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}'}
    
    def analyze_audio_file(self, audio_file: str) -> Dict:
        """
        åˆ†æéŸ³é¢‘æ–‡ä»¶æƒ…ç»ª
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.enable_audio or not self.audio_detector:
            return {'error': 'éŸ³é¢‘æ¨¡å—æœªå¯ç”¨æˆ–æœªåˆå§‹åŒ–'}
        
        try:
            result = self.audio_detector.recognizer.predict_emotion(audio_file)
            
            return {
                'modality': 'audio',
                'timestamp': time.time(),
                'emotion': self.emotion_mapping.get(result.get('emotion', 'neutral'), 'ä¸­æ€§'),
                'confidence': result.get('confidence', 0.0),
                'raw_result': result
            }
        
        except Exception as e:
            logger.error(f"éŸ³é¢‘æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return {'error': f'éŸ³é¢‘æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}'}
    
    def analyze_image_file(self, image_file: str) -> Dict:
        """
        åˆ†æå›¾åƒæ–‡ä»¶æƒ…ç»ª
        
        Args:
            image_file: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æç»“æœ
        """
        if not self.enable_vision or not self.vision_detector:
            return {'error': 'è§†è§‰æ¨¡å—æœªå¯ç”¨æˆ–æœªåˆå§‹åŒ–'}
        
        try:
            image = cv2.imread(image_file)
            if image is None:
                return {'error': 'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶'}
            
            result = self.vision_detector.analyze_image(image)
            
            if result.get('faces_detected', 0) > 0:
                face_result = result['faces'][0]
                return {
                    'modality': 'vision',
                    'timestamp': time.time(),
                    'emotion': self.emotion_mapping.get(face_result.get('emotion', 'nature'), 'ä¸­æ€§'),
                    'confidence': face_result.get('confidence', 0.0),
                    'raw_result': result
                }
            else:
                return {'error': 'æœªæ£€æµ‹åˆ°äººè„¸'}
        
        except Exception as e:
            logger.error(f"å›¾åƒåˆ†æå¤±è´¥: {e}")
            return {'error': f'å›¾åƒåˆ†æå¤±è´¥: {str(e)}'}
    
    def get_latest_result(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„èåˆç»“æœ"""
        try:
            return self.unified_results.get_nowait()
        except queue.Empty:
            return None
    
    def get_all_results(self) -> List[Dict]:
        """è·å–æ‰€æœ‰èåˆç»“æœ"""
        results = []
        while True:
            try:
                result = self.unified_results.get_nowait()
                results.append(result)
            except queue.Empty:
                break
        return results
    
    def get_status(self) -> Dict:
        """è·å–æ£€æµ‹å™¨çŠ¶æ€"""
        return {
            'is_recording': self.is_recording,
            'modules_enabled': {
                'audio': self.enable_audio,
                'vision': self.enable_vision,
                'nlp': self.enable_nlp
            },
            'modules_ready': {
                'audio': self.audio_detector is not None,
                'vision': self.vision_detector is not None and self.vision_detector.is_ready(),
                'nlp': self.nlp_pipeline is not None
            }
        }
    
    def export_results(self, output_file: str, format: str = 'json'):
        """
        å¯¼å‡ºæ‰€æœ‰ç»“æœ
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: è¾“å‡ºæ ¼å¼ ('json', 'csv')
        """
        try:
            all_results = self.get_all_results()
            
            if format == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
            
            elif format == 'csv':
                import pandas as pd
                
                # å±•å¹³ç»“æœ
                flattened_results = []
                for result in all_results:
                    flat_result = {
                        'timestamp': result['timestamp'],
                        'final_emotion': result['final_emotion'],
                        'confidence': result['confidence'],
                        'fusion_method': result['fusion_method']
                    }
                    
                    # æ·»åŠ å„æ¨¡æ€ç»“æœ
                    for modality, mod_result in result.get('modality_results', {}).items():
                        flat_result[f'{modality}_emotion'] = mod_result['emotion']
                        flat_result[f'{modality}_confidence'] = mod_result['confidence']
                    
                    flattened_results.append(flat_result)
                
                df = pd.DataFrame(flattened_results)
                df.to_csv(output_file, index=False, encoding='utf-8')
            
            logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
            
        except Exception as e:
            logger.error(f"ç»“æœå¯¼å‡ºå¤±è´¥: {e}")


def demo_unified_detection():
    """æ¼”ç¤ºç»Ÿä¸€æƒ…ç»ªè¯†åˆ«"""
    print("=" * 60)
    print("ç»Ÿä¸€å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = UnifiedEmotionDetector()
    
    # æ£€æŸ¥çŠ¶æ€
    status = detector.get_status()
    print("æ¨¡å—çŠ¶æ€:")
    for module, enabled in status['modules_enabled'].items():
        ready = status['modules_ready'][module]
        status_text = "âœ… å°±ç»ª" if enabled and ready else "âŒ æœªå°±ç»ª"
        print(f"  {module}: {status_text}")
    
    # è®¾ç½®ç»“æœå›è°ƒ
    def result_callback(result):
        print(f"\nğŸ¯ æ£€æµ‹åˆ°æƒ…ç»ª: {result['final_emotion']}")
        print(f"   ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"   æ—¶é—´: {datetime.fromtimestamp(result['timestamp']).strftime('%H:%M:%S')}")
        
        # æ˜¾ç¤ºå„æ¨¡æ€ç»“æœ
        for modality, mod_result in result.get('modality_results', {}).items():
            print(f"   {modality}: {mod_result['emotion']} ({mod_result['confidence']:.3f})")
    
    detector.set_result_callback(result_callback)
    
    print("\né€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å®æ—¶å¤šæ¨¡æ€æ£€æµ‹ï¼ˆ10ç§’ï¼‰")
    print("2. æ–‡æœ¬æƒ…ç»ªåˆ†æ")
    print("3. éŸ³é¢‘æ–‡ä»¶åˆ†æ")
    print("4. å›¾åƒæ–‡ä»¶åˆ†æ")
    
    try:
        choice = input("è¯·é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            print("\nå¼€å§‹å®æ—¶å¤šæ¨¡æ€æƒ…ç»ªæ£€æµ‹...")
            print("è¯·å¯¹ç€æ‘„åƒå¤´è¯´è¯ï¼Œç³»ç»Ÿå°†åˆ†ææ‚¨çš„è¯­éŸ³å’Œè¡¨æƒ…")
            
            detector.start_recording(duration=10)
            
            # ç­‰å¾…å½•åˆ¶å®Œæˆ
            time.sleep(11)
            
            # è·å–æ‰€æœ‰ç»“æœ
            results = detector.get_all_results()
            print(f"\næ£€æµ‹å®Œæˆï¼å…±è·å¾— {len(results)} ä¸ªèåˆç»“æœ")
            
        elif choice == '2':
            text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ")
            result = detector.analyze_text(text)
            
            if 'error' not in result:
                print(f"æ–‡æœ¬æƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            else:
                print(f"åˆ†æå¤±è´¥: {result['error']}")
        
        elif choice == '3':
            audio_file = input("è¯·è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„: ")
            result = detector.analyze_audio_file(audio_file)
            
            if 'error' not in result:
                print(f"éŸ³é¢‘æƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            else:
                print(f"åˆ†æå¤±è´¥: {result['error']}")
        
        elif choice == '4':
            image_file = input("è¯·è¾“å…¥å›¾åƒæ–‡ä»¶è·¯å¾„: ")
            result = detector.analyze_image_file(image_file)
            
            if 'error' not in result:
                print(f"å›¾åƒæƒ…ç»ª: {result['emotion']}")
                print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            else:
                print(f"åˆ†æå¤±è´¥: {result['error']}")
        
        else:
            print("æ— æ•ˆé€‰æ‹©")
    
    except KeyboardInterrupt:
        print("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
        detector.stop_recording()
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        detector.stop_recording()
    
    print("\næ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_unified_detection() 